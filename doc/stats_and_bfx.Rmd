---
title: "A Link To The Past"
subtitle: "Statistical connections within the bioinformatics toolkit"
author: "Russ Hyde"
date: "30th September, 2019"
output:
  xaringan::moon_reader:
    nature:
      highlightLines: true
---

```{r setup, include=FALSE}
library(here)
knitr::opts_chunk$set(echo = FALSE)

height.5 = 4
height.75 = 6
height.max = 8
set.seed(3)
```

```{r}
design_heatmap <- function(
  ...,
  row_title = "Samples",
  column_title = "Coefficients"
) {
  ComplexHeatmap::Heatmap(...,
    col = c("0" = "grey87", "1" = "black"),
    cluster_rows = FALSE,
    cluster_columns = FALSE,
    row_title = row_title,
    column_title = column_title
  )
}
```

```{r}
factor_model <- function(
  object, ...
) {
  stopifnot(is(object, "formula"))

  mm <- stats::model.matrix(object, ...)

  # prefixing the factor levels with the factor name seems odd

  term_names <- paste0("^", attr(terms(object), "term.labels"))

  colnames(mm) <- stringr::str_replace_all(
    colnames(mm),
    pattern = magrittr::set_names(rep("", length(term_names)), term_names)
  )

  mm
  }
```

## Preamble

Github:

- https://www.github.com/russHyde/bfx_201909

Dataset:

- [GSE103528](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE103528)

R Packages:

```{r, echo = TRUE}
# format: xaringan
pkgs <- c(
  # CRAN
  "dplyr", "magrittr", "ggplot2",
  # Bioconductor
  "ComplexHeatmap", "edgeR", "limma", "variancePartition"
)

for (pkg in pkgs) {
  suppressPackageStartupMessages(
    library(pkg, character.only = TRUE))
}
```

---

## The Dataset

[GSE103528](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE103528)

- Sources of variability:

    - 3x AML cell lines (Acute myeloid leukaemia)
    - 3x treatments ("Scramble", and two shRNAs vs _CARM1_ (aka _PRMT4_))
    - Triplicates (not obvious: are replicates batches?)

- Data:

    - Star / RSEM / GRCh38 / Ensembl-87
    - (@russHyde should have aligned the data and used actual counts
        - but life's too short
        - he just `floor`ed the RSEM values)

---

## The Dataset (cont.)

```{r, echo = TRUE, message = FALSE}
dge <- readRDS(here("data", "job", "GSE103528.dgelist.rds"))
dge <- dge[dge$genes$gene_biotype == "protein_coding", ]
dim(dge)
```

.pull-left[

```{r density, eval = FALSE, echo = TRUE}
dge %>%
  edgeR::cpm(log = TRUE) %>%
  limma::plotDensities(
    legend = FALSE,
    main = "Density of log-CPM"
    )
```
]

.pull-right[
```{r, density-out, ref.label = "density", echo = FALSE, fig.height = height.5}
```
]

---

## A Simple Class of Models (shorthand)

$$\mathbf{y} \sim \mathcal{N} \left(X\boldsymbol{\beta}, \sigma^2I \right)$$

Where,

$\mathbf{y}$ - observed values

$\mathtt{lhs} \sim \mathcal{N}(., .)$ - normality assumption (_on_ the
residuals; or _around_ the fitted values)

$X$ - _a_ design matrix (one row per observation)

$\boldsymbol{\beta}, \sigma$ - parameters to be estimated (vector and scalar,
resp)

... plus lots of assumptions

---

## ... And In Practice

$$\mathbf{y} \sim \mathcal{N} \left(X\boldsymbol{\beta}, \sigma^2I \right)$$

Where,

$\mathbf{y}$ - eg, log-expression-intensity for a single gene across a set of
samples

$\mathtt{lhs} \sim \mathcal{N}(., .)$ - some statistical distribution

$X$ - _some_ encoding of experimental design / predictors for the samples

$\boldsymbol{\beta}$ - how much does each predictor contribute ...

$\sigma$ - and how much noise is there around ...

... the fitted values

---

## Design Matrices

.pull-left[
Encode the experimental / statistical design:

- rows = samples
- columns = model coefficients

```{r, design-heatmap, eval = FALSE, echo = TRUE}
# `design_heatmap` plots binary matrices as heatmaps
# `factor_model` wraps `model.matrix` - but doesn't prefix with the factor name
design_heatmap(
  factor_model(
    ~ cell + treatment,
    data = dge$samples)
) +
  rowAnnotation(
    df = dge$samples[, c("cell", "batch", "treatment")]
  )
```

See also: ExploreModelMatrix

- https://csoneson.github.io/ExploreModelMatrix/
]

.pull-right[
```{r, design-heatmap-out, ref.label = "design-heatmap", echo = FALSE, fig.height = height.max}
```
]

---

## The implicit generalisation: transform the outcome (`lm()`)

If the residuals aren't normal(ish) in

$$\mathbf{y} \sim \mathcal{N} \left(X\boldsymbol{\beta}, \sigma^2I \right)$$
$$\downarrow$$
$$\mathbf{Y} = f(\mathbf{y})$$
$$\mathbf{Y} \sim \mathcal{N} \left(X\boldsymbol{\beta}, \sigma^2I \right)$$

---

```{r, echo = TRUE}
design <- factor_model(~ cell + treatment, data = dge$samples)
dge_voom <- voom(dge, design = design)
```

```{r, echo = FALSE}
row_carm1 <- dge_voom$genes$gene_name == "CARM1"

df_carm1 <- data.frame(
  intensity = dge_voom$E[row_carm1, ],
  dge$samples
)

p_carm1 <- df_carm1 %>%
  ggplot(
    aes(x = treatment, y = intensity)
  ) +
  geom_boxplot() +
  facet_wrap(~ cell) +
  labs(
    y = "Normalised CARM1 intensity (log2)")
```

```{r, fig.height = height.5, fig.pos = "center"}
p_carm1
```

---

## `lm()` (cont.)

```{r}
lm_carm1 <- lm(intensity ~ cell + treatment, data = df_carm1)
lm_carm1
```

Baselines (for cell: MOLM13; for treatment: sh55)

Fitted values:

- [SKNO1, shControl]:

    - (Intercept) + cellskno1 + treatmentsshct

```{r, echo = TRUE}
with(
  lm_carm1,
  sum(coefficients[c("(Intercept)", "cellskno1", "treatmentshct")]))
```

---

But
- `design` is a N x K matrix (here, binary)
- `coefficients` is K x 1 vector (entries ~ columns of the design)

What is the matrix * vector product?

```{r, echo = TRUE}
(
  design %*% lm_carm1$coefficients
  ) %>%
  tail(n = 12)
```

---

## Equivalent Design Matrices

```{r}
hm1 <- design_heatmap(
  factor_model(~ cell + treatment, data = dge$samples),
  column_title = "~ cell + treatment[baseline:sh55]",
  name = "design_1"
)
```

```{r}
ra <- rowAnnotation(df = dge$samples[, c("cell", "batch", "treatment")])
```

```{r}
hm2 <- design_heatmap(
  factor_model(
    ~ -1 + cell + treatment,
    data = dplyr::mutate(
      dge$samples, treatment = relevel(factor(treatment), "shct"))
    ),
  column_title = "~ -1 + cell + treatment[baseline:shct]",
  name = "design_2"
)
```

```{r, fig.width = 10, fig.height = height.75}
draw(hm1 + ra + hm2, ht_gap = unit(3, "cm"))

f <- function(i0) {
  i <- i0
  y <- c(i0 - 1, i0) / nrow(dge$samples)
  grid.lines(c(0, 1), y[1], gp = gpar(col = "red"))
  grid.lines(c(0, 1), y[2], gp = gpar(col = "red"))
}

decorate_heatmap_body("design_1", f(7))
decorate_heatmap_body("design_2", f(7))
```

LHS `skno1` != RHS `skno1` coef

---

## The 'borrow information' generalisation (`limma`)

$$\mathbf{y}_{[g_1]} \sim \mathcal{N} \left(X\boldsymbol{\beta}_{[g_1]}, \sigma^2_{[g_1]}I \right)$$
$$\mathbf{y}_{[g_2]} \sim \mathcal{N} \left(X\boldsymbol{\beta}_{[g_2]}, \sigma^2_{[g_2]}I \right)$$
$$\downarrow$$

$$\sigma_{g} : {balance\ between\ gene-specific\ estimate\ and\ global\ estimate}$$
$$\mathbf{y}_{[g]} \sim \mathcal{N} \left(X\boldsymbol{\beta}_{[g]}, \sigma^2_{[g]}I \right)$$


---

```{r, echo = TRUE, warning = FALSE}
colnames(design)
```

```{r, echo = TRUE, warning = FALSE}
contrasts <- limma::makeContrasts(
  # baseline treatment is sh55, so (fit for sh55) - (fit for shCTRL)
  # is (0) - (shct)
  sh_55 = "-shct",
  sh_57 = "sh57 - shct",
  sh_average = "(sh57 - 2*shct) / 2",
  levels = design
)
```

```{r, echo = TRUE, warning = FALSE}
# standard limma workflow
# - vanilla linear model
fit_raw <- limma::lmFit(dge_voom, design)
# - estimate experimental contrasts
fit_cont <- limma::contrasts.fit(fit_raw, contrasts = contrasts)
# - moderated t-statistics (uses distribution of standard
# errors for all features to better estimate that for a
# given feature)
fit_ebayes <- limma::eBayes(fit_cont)
```

---

## ... 'information borrowing' does not affect 'fold-change'

... but voom-contrasts `!=` the `lm()` estimates

```{r}
rbind(
  fit_raw$coefficients[row_carm1, ] %*% contrasts,
  fit_ebayes$coefficients[row_carm1, ],
  lm_carm1$coefficients %*% contrasts
) %>% set_rownames(
  c("after limma::lmFit", "after limma::eBayes", "from `lm()`")
)
```

--

"voom: Precision **weights** unlock linear model analysis tools for RNA-seq read counts." Law et al (Genome Biol. 2014)


```{r, echo = TRUE, tidy = FALSE}
with(
  dge_voom[row_carm1, ],
    lm.wfit(x = design, y = as.vector(E), w = as.vector(weights)) #<<
)$coefficients %*% contrasts
```

---

## `limma` does other stuff ...

```{r, echo = FALSE}
limma_fig <- here(
  "figures",
  "gkv007fig1.jpg"
)

limma_fig_caption <- "Source: Richie et al. Nuc. Acids Res. (2015)"
```


```{r limma, echo = FALSE, fig.cap=limma_fig_caption, out.width = '75%'}
knitr::include_graphics(limma_fig)
```
---

## Some Univariate Distibutions

```{r, echo = FALSE}
# source: wikipedia
url <- here(
  "figures",
  "Relationships_among_some_of_univariate_probability_distributions.jpg"
)

figure_caption <- "Source: Ehsan Azhdari [CC BY-SA 3.0] via Wikipedia"
```

```{r distributions, echo=FALSE, fig.cap=figure_caption, out.width = '100%'}
knitr::include_graphics(url)
```

---

## The 'why should the residuals be Normal?' generalisation: (`edgeR` / `DESeq2`)

$$\mathbf{y} \sim \mathcal{N} \left(X\boldsymbol{\beta}, \sigma^2I \right)$$

that is, for the $i$th observation:

$$y_i \sim \mathcal{N} \left(\Sigma_j X_{ij}\beta_j, \sigma^2 \right)$$

$$\downarrow$$

$$y_i \sim \mathcal{\psi} \left(\Sigma_j X_{ij}\beta_j,\ other\ params \right)$$

eg, [generalised linear models](
  https://newonlinecourses.science.psu.edu/stat504/node/216/
)

---

## RNA-Seq counts are _counts_

Count distributions (examples)

- {0, 1} Bernoulli
- {0, 1, 2, ..., n} Binomial
- {0, 1, 2, ..., Inf} Poisson, Geometric

--

Negative-Binomial $\mathcal{NB} \left( r, q \right)$

- used in both `edgeR` and `DESeq2`

- generalisation of
    - Poisson
    - Geometric

- practically: just more flexible than Poisson

- many ways of parameterising:
    - r = size, q = prob of success
    - m = mean = $r(1-q)/q$ , v = variance = $r(1-q)/q^2$

---

## The `edgeR` model

For a gene $g$ and sample $i$ (from experimental group $k$)

$$y_i \sim \mathcal{NB}\left( mean = m_i, var = v_i \right)$$

where

- $m_i = M_i p_k$

    - $M_i$ = library size

    - $p_k$ = relative abundance of $g$ in experimental group $k$

- $v_i = m_i\left( 1 + m_i \phi \right)$

- $p_k = \exp\left( \Sigma_j {X_{ij}\beta_j} \right)$

- $y_i$ is the observed count

- `edgeR` call $\phi$ the dispersion parameter (note, $\phi = 0$ is a Poisson
model; note $\phi ^ {1/2} = BCV$)

---

```{r, echo = TRUE}
dge_edger <- edgeR::estimateDisp(dge, design = design, tagwise = TRUE)

fit_edger <- edgeR::glmFit(dge_edger, design = design)
```

```{r, echo = TRUE}
# convert coefs to log2
fit_edger$coefficients[row_carm1, ] * log2(exp(1))
```

```{r}
rbind(
  lm_carm1$coefficients %*% contrasts,
  fit_ebayes$coefficients[row_carm1, ],
 (fit_edger$coefficients[row_carm1, ] * log2(exp(1))) %*% contrasts
) %>% set_rownames(
  c("`lm()`", "`limma::eBayes`", "`edgeR::glmFit`")
)
```

---

To get (approx) the same coefficients as `edgeR` using `glm`

```{r, echo = TRUE}
model_data <- with(
  dge_edger,
  data.frame(
    counts = as.vector(counts[row_carm1, ]),
    samples
))
```

```{r, echo = TRUE}
nb <- glm(
  # note that the `offset()` introduces a fixed constant (it's included in the
  # model with a coefficient of 1)
  counts ~ offset(log(lib.size)) + cell + treatment,
  data = model_data,
  family = MASS::negative.binomial(
    # R/MASS uses theta = 1 / phi, where phi is `edgeR`'s dispersion parameter
    # note that v = mu + mu^2/theta = mu + phi * mu^2
    theta = 1 / dge_edger$tagwise.dispersion[row_carm1]
  )
)
```

---

```{r, echo = TRUE}
nb$coefficients * log2(exp(1))
(nb$coefficients * log2(exp(1))) %*% contrasts
```

---

## The 'why should all the effects be _fixed_?' generalisation: (`lme4`, `variancePartition`)

$$\mathbf{y} \sim X\boldsymbol{\beta} + \mathcal{N} \left(\boldsymbol{0}, \sigma^2I \right)$$
that is,

$$\mathbf{y} \sim {Fixed\ Effects} + Noise$$
$$\downarrow$$
$$\mathbf{y} \sim [Fixed Effects] + [Random Effects] + Noise$$

---

Sources of variability in the current experiment:

- Cell type

- Batch (not modelled here)

- Treatment

- Technical issues, noise etc

Fixed effect:

- Unknown _constant_ we are trying to estimate

---

## Why are we estimating the baseline difference between cell types?

```{r, fig.height = height.5}
design_heatmap(design) +
  rowAnnotation(
    df = dge$samples[, c("cell", "batch", "treatment")]
  )
```

Random effect:

- A model parameter that is a random variable

- You estimate the properties of its distribution, not its _value_

---

## Mixed Model version

```{r}
treatment_design <- model.matrix(~ treatment, data = dge$samples)
cell_design <- model.matrix(~ -1 + cell, data = dge$samples)

design_heatmap(
  treatment_design, show_heatmap_legend = FALSE, column_title = "Fixed effects"
) +
  rowAnnotation(foo = anno_empty(border = FALSE,
    width = max_text_width("X beta + ") + unit(4, "mm"))
  ) +
  design_heatmap(
    cell_design, show_row_names = FALSE, show_heatmap_legend = FALSE, column_title = "Random effects"
  ) +
  rowAnnotation(bar = anno_empty(border = FALSE,
    width = max_text_width("X gamma + Noise") + unit(4, "mm"))
  )
decorate_annotation("foo", grid.text("X beta +"))
decorate_annotation("bar", grid.text("X gamma + Noise"))
```

---

## `voom::duplicateCorrelation`

```{r, echo = TRUE, message = FALSE}
# limma can accomodate at most one random effect
treatment_design <- model.matrix(
  ~ -1 + treatment, data = dge_voom$targets
)
treatment_contrast <- makeContrasts(
  sh55 = "treatmentsh55 - treatmentshct",
  sh57 = "treatmentsh57 - treatmentshct",
  levels = treatment_design
)
# 'random effects' as used in limma
blocks <- factor(dge_voom$targets$cell)
```

---

## `duplicateCorrelation (cont.)`

```{r, echo = TRUE}
dupcor <- limma::duplicateCorrelation(
  dge_voom, treatment_design, block = blocks)

fit_cor <- lmFit(
  dge_voom, treatment_design,
  block = blocks, correlation = dupcor$consensus #<<
)
fit_cor$coefficients[row_carm1, ] %*% treatment_contrast
```

---

## `variancePartition`

```{r, echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
vp <- variancePartition::dream(
  dge_voom[row_carm1, ], # slow for all genes; recommend parallel
  formula = ~ -1 + treatment + (1|cell), # lme4 model syntax #<<
  data = dge_voom$targets,
  L = treatment_contrast
)
```

```{r, echo = TRUE}
vp$coefficients
```

---

## Multiple random effects in `variancePartition::dream`

```{r, echo = TRUE, message = FALSE}
vp2 <- variancePartition::dream(
  dge_voom[row_carm1, ],
  formula = ~ -1 + treatment + (1|cell) + (1|batch), #<<
  data = dge_voom$targets,
  L = treatment_contrast
)
```

```{r, echo = TRUE}
vp2$coefficients
```

---

## The 'beyond the scope of the talk, and of my explanatory powers' generalisations ...

- fully Bayesian approaches (STAN etc)

- genes aren't independent of each other ...

    - epigenomic context

    - coregulatory modules

- dynamic modelling of gene expression ...

- the cell population milkshake ...

---

## Further Reading

Connections between different models

- Faraway - [Extending the linear model with R](https://people.bath.ac.uk/jjf23/ELM/)
- Dobson & Barnett - [An Introduction to Generalized Linear Models](
  https://www.crcpress.com/An-Introduction-to-Generalized-Linear-Models/Dobson-Barnett/p/book/9781138741515)

Bayesian stats (beyond the main part of this talk)

- McElreath - [Statistical Rethinking](https://xcelab.net/rm/statistical-rethinking/)

---

# Appendix

<!--

## The Simplest Model (Assumptions)

Residuals:

- IID normal

- mean 0 for all values of predictor variables

Model:

- The model is correct ...

- Predictors are fixed and have no errors

- Single source of unaccounted noise

-->

<!--
  - How should the dependent variable be modelled
  - How should the explanatory / independent variables be incorporated
  - What should be estimated
  - What should be controlled
  - How should the covariates be controlled
  - How should uncertainties in mapping, biases in library composition etc etc
  be incorporated
-->

<!--
  Illustrate with a single gene from the example dataset
-->

<!--
  There is _no_ one simplest model
-->

---

## NB = Gamma-Poisson Mixture

To sample 1000 NB(r, p) values
- sample 1000 Gamma(shape = r, scale = p/(1-p)) values
- then for each Gamma value, g, sample a Poisson(rate = g)
- The latter are NB(r, p) // but R uses 1 - p rather than p

```{r}
set.seed(1)
r <- 4
p <- 0.25
```

```{r}
n <- 1000
lambdas <- rgamma(n = n, shape = r, scale = p / (1 - p))
nb_samples <- rpois(n = n, lambda = lambdas)
```

```{r, fig.height = height.5}
# https://mikelove.wordpress.com/2011/03/30/r-one-liner-histogram-for-integers/
int.hist = function(x, ylab="Density", ...) {
  barplot(
    table(factor(x, levels = min(x) : max(x))) / length(x),
    space = 0,
    xaxt = "n",
    ylab = ylab,
    ...
  )
  axis(1)
}

par(mfrow = c(1, 2))
hist(lambdas, probability = TRUE, col = "grey", xlab = "lambda")
lines(seq(0, 5, 0.01), dgamma(seq(0, 5, 0.01), shape = r, scale = p / (1 - p)))

int.hist(nb_samples, main = "Histogram of NB(r, p)")
points(0.5 + 0:7, dnbinom(0:7, size = r, prob = 1 - p))
```

---

## `edgeR` fits larger coefficients for poorly-detected genes than `limma`

```{r, fig.height = height.75}
data.frame(
  limma = fit_ebayes$coefficients[, "sh_55"],
  edger = (fit_edger$coefficients * log2(exp(1))) %*% contrasts[, "sh_55"],
  log_cpm = aveLogCPM(dge_edger)
) %>%
  mutate(
    cpm_group = cut(log_cpm,
                    breaks = quantile(log_cpm, probs = seq(0, 1, 1/3)),
                    include.lowest = TRUE)
  ) %>%
ggplot(
  aes(x = limma, y = edger)
  ) +
  facet_wrap(~ cpm_group) +
  geom_point(alpha = 1 / 10) +
  scale_fill_continuous(type = "viridis") +
  lims(x = c(-5, 5), y = c(-5, 5))
```

---

# Using `stan` to fit a negative binomial model for RNA-Seq

```{r, echo = TRUE}
suppressPackageStartupMessages(
  library(rstan)
)
```

---

## stan (cont.)

```{r, echo = TRUE}
# modified from http://rstudio-pubs-static.s3.amazonaws.com/34099_2e35c3966ef548c2918d5b6c2146bfd1.html

model <- "
// negative binomial parameterized as eta (log(mu)) and dispersion (phi)
// see p286 in stan-reference-2.4.0.pdf
// a basic GLM example
data {
  int<lower=1> N;    // rows of data
  matrix[N, 5] X;    // predictor
  vector[N] L;       // lib sizes
  int<lower=0> y[N]; // response
  real phi;          // edgeR dispersion parameter (glm() and stan use the
                     //   inverse of this to specify mean/variance relationship;
                     //   we call the inverse `theta` for consistency with glm())
                     // Apologies for the confusion, stan's docs refer to our
                     //   `theta` as `phi`
}
parameters {
  vector[5] beta; // coefficients for the linear component
}
transformed parameters {
  vector[N] y_hat = L .* exp(X * beta); // linear component
  real theta = 1 / phi;
}
model {
  // data model:
  y ~ neg_binomial_2(y_hat, theta);
}
"
```

---

## stan (cont.)

```{r, echo = TRUE}
stan_data <- list(
  N = ncol(dge_edger),
  y = as.vector(dge_edger$counts[row_carm1, ]),
  L = as.vector(dge_edger$samples$lib.size),
  X = set_colnames(design, c("intercept", colnames(design)[-1])),
  phi = dge_edger$tagwise.dispersion[row_carm1]
)
```

```{r, echo = TRUE}
write(model, file = here("models/negbin-glm.stan"))
```

```{r, echo = TRUE}
sm <- stan_model(here("models/negbin-glm.stan"))
```

---

## stan (cont.)

```{r, message = FALSE, warning = FALSE, echo = TRUE}
# refresh = 0 prevents sampling output from printing
m <- sampling(sm, data = stan_data,
  pars = c("beta"),
  iter = 2000, chains = 4, verbose = FALSE, refresh = 0)
```

```{r, echo = TRUE}
log2(exp(1)) *
  rbind(
    edgeR = fit_edger$coefficients[row_carm1, ],
    glm = nb$coefficients,
    stan = summary(m, "beta")$summary[, "mean"]
) %*% contrasts
```

<!--

## The Statistical Philosophies

-->
